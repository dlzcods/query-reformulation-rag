{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62f1126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from firecrawl import Firecrawl\n",
    "import os\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "API_KEY = os.getenv(\"FIRECRAWL_API_KEY\", \"fc-64ba4495945740eeb2a648e81c05b6c5\")\n",
    "fc = Firecrawl(api_key=API_KEY)\n",
    "\n",
    "def extract_article_links(list_url):\n",
    "    print(\"Fetching list page:\", list_url)\n",
    "    doc = fc.scrape(list_url, formats=[\"html\"])\n",
    "    html = doc.html\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    links = []\n",
    "    \n",
    "    for a in soup.select(\"a[href]\"):\n",
    "        href = a.get(\"href\")\n",
    "        text = a.get_text(strip=True)\n",
    "        if not href or not text:\n",
    "            continue\n",
    "        if \"/klinik/a/\" in href:\n",
    "            if href.startswith(\"/\"):\n",
    "                href = \"https://www.hukumonline.com\" + href\n",
    "            links.append({\"title\": text, \"link\": href})\n",
    "    \n",
    "    seen = set()\n",
    "    unique_links = []\n",
    "    for item in links:\n",
    "        if item[\"link\"] not in seen:\n",
    "            seen.add(item[\"link\"])\n",
    "            unique_links.append(item)\n",
    "    \n",
    "    print(f\"Found {len(unique_links)} unique links on {list_url}\")\n",
    "    return unique_links\n",
    "\n",
    "def extract_publish_date(soup):\n",
    "    \"\"\"Extract publish date from the article page\"\"\"\n",
    "    # Coba beberapa selector untuk tanggal\n",
    "    \n",
    "    # 1. Cari di metadata\n",
    "    date_meta = soup.find(\"meta\", property=\"article:published_time\")\n",
    "    if date_meta and date_meta.get(\"content\"):\n",
    "        return date_meta.get(\"content\")\n",
    "    \n",
    "    # 2. Cari di structured data (JSON-LD)\n",
    "    script_tags = soup.find_all(\"script\", type=\"application/ld+json\")\n",
    "    for script in script_tags:\n",
    "        try:\n",
    "            import json\n",
    "            data = json.loads(script.string)\n",
    "            if isinstance(data, dict):\n",
    "                if \"datePublished\" in data:\n",
    "                    return data[\"datePublished\"]\n",
    "                if \"publishedTime\" in data:\n",
    "                    return data[\"publishedTime\"]\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # 3. Cari di elemen dengan class/id yang mengandung tanggal\n",
    "    date_elem = soup.find(\"time\")\n",
    "    if date_elem:\n",
    "        return date_elem.get(\"datetime\") or date_elem.get_text(strip=True)\n",
    "    \n",
    "    # 4. Cari pattern tanggal dalam teks (format: DD MMM, YYYY)\n",
    "    date_pattern = re.compile(r'\\d{1,2}\\s+(?:Jan|Feb|Mar|Apr|Mei|Jun|Jul|Agu|Sep|Okt|Nov|Des)[a-z]*,?\\s+\\d{4}')\n",
    "    text = soup.get_text()\n",
    "    match = date_pattern.search(text)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def extract_article_content(html):\n",
    "    \"\"\"Extract clean article content from HTML\"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    # Cari wrapper utama yang berisi konten artikel\n",
    "    main_wrapper = soup.select_one(\"div.css-103zlhi.elbhtsw0\")\n",
    "    \n",
    "    if main_wrapper:\n",
    "        # Hapus elemen yang tidak diinginkan HANYA di dalam wrapper ini\n",
    "        unwanted_selectors = [\n",
    "            \"article.css-1eyd3st.ejhsnq53\",  # KLINIK TERKAIT section\n",
    "            \"div.css-ukcqzp\",  # Iklan kursus online\n",
    "            \"div.css-uk4b7z\",  # Iklan in-article\n",
    "            \"div.adunitContainer\",  # Container iklan\n",
    "            \".swiper\",  # Carousel\n",
    "            \"iframe\",  # Iframes\n",
    "        ]\n",
    "        \n",
    "        for selector in unwanted_selectors:\n",
    "            for elem in main_wrapper.select(selector):\n",
    "                elem.decompose()\n",
    "        \n",
    "        # Cari div yang berisi konten artikel (css-15rxf41)\n",
    "        content_div = main_wrapper.select_one(\"div.css-15rxf41.e1vjmfpm0\")\n",
    "        \n",
    "        if content_div:\n",
    "            # Ambil semua teks dari content_div\n",
    "            text = content_div.get_text(separator=\"\\n\", strip=True)\n",
    "            \n",
    "            # Bersihkan teks yang tidak diinginkan\n",
    "            text = re.sub(r'KLINIK TERKAIT.*?(?=\\n[A-Z]|\\n\\n|$)', '', text, flags=re.DOTALL)\n",
    "            text = re.sub(r'Belajar Hukum Secara Online.*?Lihat Semua Kelas\\s*', '', text, flags=re.DOTALL)\n",
    "            text = re.sub(r'Navigate (left|right)\\s*', '', text, flags=re.IGNORECASE)\n",
    "            \n",
    "            # Bersihkan whitespace berlebih\n",
    "            text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "            \n",
    "            return text.strip()\n",
    "    \n",
    "    # Fallback 1: Cari langsung content wrapper\n",
    "    wrapper = soup.select_one(\"div.css-15rxf41.e1vjmfpm0\")\n",
    "    if wrapper:\n",
    "        # Hapus elemen tidak diinginkan\n",
    "        for selector in [\"article.css-1eyd3st\", \"div.css-ukcqzp\", \"div.css-uk4b7z\"]:\n",
    "            for elem in wrapper.select(selector):\n",
    "                elem.decompose()\n",
    "        \n",
    "        text = wrapper.get_text(separator=\"\\n\", strip=True)\n",
    "        text = re.sub(r'KLINIK TERKAIT.*?(?=\\n[A-Z]|\\n\\n|$)', '', text, flags=re.DOTALL)\n",
    "        text = re.sub(r'Belajar Hukum Secara Online.*?Lihat Semua Kelas\\s*', '', text, flags=re.DOTALL)\n",
    "        return text.strip()\n",
    "    \n",
    "    # Fallback 2: Ambil semua teks\n",
    "    return soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "def scrape_full_article(article):\n",
    "    \"\"\"Scrape full article with all metadata\"\"\"\n",
    "    url = article[\"link\"]\n",
    "    print(\"Scraping article:\", url)\n",
    "    \n",
    "    doc = fc.scrape(url, formats=[\"html\"])\n",
    "    html = doc.html if hasattr(doc, \"html\") else None\n",
    "    \n",
    "    content_clean = None\n",
    "    publish_date = None\n",
    "    \n",
    "    if html:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        content_clean = extract_article_content(html)\n",
    "        publish_date = extract_publish_date(soup)\n",
    "    else:\n",
    "        content_clean = getattr(doc, \"markdown\", \"\")\n",
    "    \n",
    "    # Fallback untuk tanggal dari metadata Firecrawl\n",
    "    if not publish_date and hasattr(doc, \"metadata\"):\n",
    "        meta = doc.metadata\n",
    "        if hasattr(meta, \"published_at\"):\n",
    "            publish_date = meta.published_at\n",
    "        elif isinstance(meta, dict):\n",
    "            publish_date = meta.get(\"published_at\") or meta.get(\"publishedTime\")\n",
    "    \n",
    "    return {\n",
    "        \"title\": article[\"title\"],\n",
    "        \"link\": url,\n",
    "        \"publish_date\": publish_date,\n",
    "        \"content\": content_clean\n",
    "    }\n",
    "\n",
    "def scrape_pages(start_page=1, end_page=2):\n",
    "    \"\"\"Scrape multiple pages of articles\"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    for page in range(start_page, end_page + 1):\n",
    "        if page == 1:\n",
    "            list_url = \"https://www.hukumonline.com/klinik/perdata/\"\n",
    "        else:\n",
    "            list_url = f\"https://www.hukumonline.com/klinik/perdata/page/{page}/\"\n",
    "        \n",
    "        article_links = extract_article_links(list_url)\n",
    "        \n",
    "        for art in article_links:\n",
    "            try:\n",
    "                data = scrape_full_article(art)\n",
    "                all_results.append(data)\n",
    "                print(f\"✓ Scraped: {data['title']}\")\n",
    "                print(f\"  Date: {data['publish_date']}\")\n",
    "                time.sleep(2)\n",
    "            except Exception as ex:\n",
    "                print(f\"✗ Error scraping {art['link']}: {ex}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = scrape_pages(start_page=1, end_page=1)\n",
    "    \n",
    "    import json\n",
    "    with open(\"hukumonline_perdata_articles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\n✓ Done! Total articles scraped: {len(results)}\")\n",
    "    print(f\"  Saved to: hukumonline_perdata_articles.json\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
